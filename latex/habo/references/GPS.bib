@misc{friedmanGaussianProcessNetworks2013,
  title = {Gaussian {{Process Networks}}},
  author = {Friedman, Nir and Nachman, Iftach},
  year = 2013,
  month = jan,
  number = {arXiv:1301.3857},
  eprint = {1301.3857},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1301.3857},
  urldate = {2024-11-19},
  abstract = {In this paper we address the problem of learning the structure of a Bayesian network in domains with continuous variables. This task requires a procedure for comparing different candidate structures. In the Bayesian framework, this is done by evaluating the \textbraceleft em marginal likelihood/\textbraceright{} of the data given a candidate structure. This term can be computed in closed-form for standard parametric families (e.g., Gaussians), and can be approximated, at some computational cost, for some semi-parametric families (e.g., mixtures of Gaussians). We present a new family of continuous variable probabilistic networks that are based on \textbraceleft em Gaussian Process/\textbraceright{} priors. These priors are semi-parametric in nature and can learn almost arbitrary noisy functional relations. Using these priors, we can directly compute marginal likelihoods for structure learning. The resulting method can discover a wide range of functional dependencies in multivariate data. We develop the Bayesian score of Gaussian Process Networks and describe how to learn them from data. We present empirical results on artificial data as well as on real-life domains with non-linear dependencies.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/joe/Zotero/storage/U3WXH4F6/Friedman and Nachman - 2013 - Gaussian Process Networks.pdf;/Users/joe/Zotero/storage/BCB5ILYU/1301.html}
}

@article{kennedyPredictingOutputComplex2000,
  title = {Predicting the Output from a Complex Computer Code When Fast Approximations Are Available},
  author = {Kennedy, {\relax MC} and O'Hagan, A},
  year = 2000,
  month = mar,
  journal = {Biometrika},
  volume = {87},
  number = {1},
  pages = {1--13},
  issn = {0006-3444},
  doi = {10.1093/biomet/87.1.1},
  urldate = {2025-06-25},
  abstract = {We consider prediction and uncertainty analysis for complex computer codes which can be run at different levels of sophistication. In particular, we wish to improve efficiency by combining expensive runs of the most complex versions of the code with relatively cheap runs from one or more simpler approximations. A Bayesian approach is described in which prior beliefs about the codes are represented in terms of Gaussian processes. An example is presented using two versions of an oil reservoir simulator.},
  file = {/Users/joe/Zotero/storage/EIKMDCES/Kennedy and O'Hagan - 2000 - Predicting the output from a complex computer code when fast approximations are available.pdf;/Users/joe/Zotero/storage/7NF8CABG/87.1.html}
}

@book{PatternRecognitionMachine,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  urldate = {2024-11-19},
  langid = {english},
  file = {/Users/joe/Zotero/storage/Y6BMU7Z5/9780387310732.html}
}

@article{perdikarisNonlinearInformationFusion2017,
  title = {Nonlinear Information Fusion Algorithms for Data-Efficient Multi-Fidelity Modelling},
  author = {Perdikaris, P. and Raissi, M. and Damianou, A. and Lawrence, N. D. and Karniadakis, G. E.},
  year = 2017,
  month = feb,
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {473},
  number = {2198},
  pages = {20160751},
  issn = {1364-5021},
  doi = {10.1098/rspa.2016.0751},
  urldate = {2026-01-27},
  abstract = {Multi-fidelity modelling enables accurate inference of quantities of interest by synergistically combining realizations of low-cost/low-fidelity models with a small set of high-fidelity observations. This is particularly effective when the low- and high-fidelity models exhibit strong correlations, and can lead to significant computational gains over approaches that solely rely on high-fidelity models. However, in many cases of practical interest, low-fidelity models can only be well correlated to their high-fidelity counterparts for a specific range of input parameters, and potentially return wrong trends and erroneous predictions if probed outside of their validity regime. Here we put forth a probabilistic framework based on Gaussian process regression and nonlinear autoregressive schemes that is capable of learning complex nonlinear and space-dependent cross-correlations between models of variable fidelity, and can effectively safeguard against low-fidelity models that provide wrong trends. This introduces a new class of multi-fidelity information fusion algorithms that provide a fundamental extension to the existing linear autoregressive methodologies, while still maintaining the same algorithmic complexity and overall computational cost. The performance of the proposed methods is tested in several benchmark problems involving both synthetic and real multi-fidelity datasets from computational fluid dynamics simulations.},
  file = {/Users/joe/Zotero/storage/4G9AKSEP/Perdikaris et al. - 2017 - Nonlinear information fusion algorithms for data-efficient multi-fidelity modelling.pdf;/Users/joe/Zotero/storage/VPDZ2VII/rspa.2016.html}
}

@book{rasmussenGaussianProcessesMachine2006,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = 2006,
  series = {Adaptive Computation and Machine Learning},
  publisher = {MIT Press},
  address = {Cambridge, Mass},
  isbn = {978-0-262-18253-9},
  langid = {english},
  lccn = {QA274.4 .R37 2006},
  keywords = {Data processing,Gaussian processes,Machine learning,Mathematical models},
  annotation = {OCLC: ocm61285753},
  file = {/Users/joe/Zotero/storage/5L6CCMS3/Rasmussen and Williams - 2006 - Gaussian processes for machine learning.pdf}
}

@misc{wilsonGaussianProcessRegression2011,
  title = {Gaussian {{Process Regression Networks}}},
  author = {Wilson, Andrew Gordon and Knowles, David A. and Ghahramani, Zoubin},
  year = 2011,
  month = oct,
  number = {arXiv:1110.4411},
  eprint = {1110.4411},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1110.4411},
  urldate = {2024-11-19},
  abstract = {We introduce a new regression framework, Gaussian process regression networks (GPRN), which combines the structural properties of Bayesian neural networks with the non-parametric flexibility of Gaussian processes. This model accommodates input dependent signal and noise correlations between multiple response variables, input dependent length-scales and amplitudes, and heavy-tailed predictive distributions. We derive both efficient Markov chain Monte Carlo and variational Bayes inference procedures for this model. We apply GPRN as a multiple output regression and multivariate volatility model, demonstrating substantially improved performance over eight popular multiple output (multi-task) Gaussian process models and three multivariate volatility models on benchmark datasets, including a 1000 dimensional gene expression dataset.},
  archiveprefix = {arXiv},
  keywords = {Quantitative Finance - Statistical Finance,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/joe/Zotero/storage/2SNHJA25/Wilson et al. - 2011 - Gaussian Process Regression Networks.pdf;/Users/joe/Zotero/storage/E5DN8LDB/1110.html}
}
